
\documentclass[sigplan,review]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{color}
\usepackage{stmaryrd}  % double brackets
\usepackage{listings}
\lstset{
  mathescape=true
}
\usepackage{amssymb}
\usepackage{enumitem}
\setlist[itemize,1]{itemsep=4pt}

\renewcommand{\arraystretch}{1.2}

\renewcommand{\to}{\rightarrow}    % ->
\newcommand{\linto}{\multimap}     % -o
\newcommand{\grad}[1]{\nabla_S\lb #1 \rb}  % grad[#1]
\newcommand{\gradf}[1]{\nabla\! \mathit{#1}}  % Full Jacobian
\newcommand{\gradft}[1]{\Delta\mathit{#1}}  % Full Jacobian
\newcommand{\fwdDf}[1]{f'}  % Forward derivative, f'
\newcommand{\revDf}[1]{f`}  % Reverse derivative, f`
\newcommand{\lb}{\llbracket}
\newcommand{\rb}{\rrbracket}
\newcommand{\sel}[2]{\pi_{#1,#2}}
\newcommand{\iffun}{\mathit{if}}
\newcommand{\buildfun}{\mathit{build}}
\newcommand{\sumfun}{\Sigma}
\newcommand{\sizefun}{size}
\newcommand{\deltafun}{\delta}
\newcommand{\indexfun}[2]{#1[#2]}   % Vector indexing
\renewcommand{\vector}[1]{\mathit{Vec}\;#1}

\newcommand{\typ}[2]{#1 \! : \! #2}  % x:S, with less horizontal whitespace

\renewcommand{\dot}{.\,}               % dot with some space after
\newcommand{\real}{\mathbb{R}}       % R, the reals
\newcommand{\nat}{\mathbb{N}}        % N, the natural numbers
\newcommand{\darrow}{\Rightarrow}    % =>

% Linear maps
\newcommand{\lmapply}{\odot}   % Infix application\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\lmtrans}[1]{#1^{\top}}   % Trnaspose (takes an argument)
\newcommand{\lmpair}{\times}         % Infix pairing
\newcommand{\lmjoin}{\bowtie}        % Infix join
\newcommand{\lmadd}{\oplus}        % Infix join
\newcommand{\lmzero}{\mathbf{0}}     % 0
\newcommand{\lmone}{\mathbf{1}}      % 1
\newcommand{\lmscalar}[1]{{\mathcal S}(#1)}      % S(k)
\newcommand{\lmlam}[1]{{\mathcal L}(#1)}      % L(k)
\newcommand{\lmlamt}[1]{{\mathcal M}(#1)}     % Transposed lambda
\newcommand{\lmbuild}{\mathcal B}             % Build linear map
\newcommand{\lmbuildt}{\mathcal C}             % Transposed Build linear map

\newcommand{\simon}[1]{{\color{red}SPJ: #1}}
\begin{document}

\title{Automatic differentation in Coconut}
\author{Tom Ellis}
\author{Simon Peyton Jones}
\author{Andrew Fitzgibbon}

\maketitle

% ------------------------------------
\section{Linear maps}

\begin{figure*}
\fbox{\begin{minipage}{\textwidth}
  $$
  \begin{array}{rr@{\hspace{2mm}}c@{\hspace{2mm}}ll}
      & \multicolumn{3}{l}{\mbox{Operator \hspace{2em} Type}} & \mbox{Matrix interpretation} \\
      & &&& \mbox{where $s = \real^m$, and $t = \real^n$} \\
\hline
    \mbox{Apply} & (\lmapply) & : & (s \linto t) \to (s \to t)
           & \mbox{Matrix/vector multiplication} \\
    \mbox{Compose} & (\lmcomp) & : & (s \linto t,\; r \linto s) \to (r \linto t)
           & \mbox{Matrix/matrix multiplication} \\
    \mbox{Sum}   & (\lmadd) & : & Field\; t \darrow (s \linto t,\; s \linto t) \to (s \linto t)
           & \mbox{Matrix addition } \\
    \mbox{Zero}  & \lmzero & : & Field\; t \darrow s \linto t & \mbox{Zero matrix}\\
    \mbox{Unit}  & \lmone  & : & s \linto s & \mbox{Identity matrix (square)}\\
    \mbox{Scale} & \lmscalar{\cdot} & : & Field\; s \darrow s \to (s \linto s) \\
    \mbox{Pair}      & (\lmpair) & : & Field\; s \darrow (s \linto t_1,\; s \linto t_2) \to (s \linto (t_1,t_2))
           & \mbox{Vertical juxtaposition} \\
    \mbox{Join}  & (\lmjoin) & : & Field\; s \darrow (t_1 \linto s,\; t_2 \linto s) \to ((t_1,t_2) \linto s)
           & \mbox{Horizontal juxtaposition} \\
    \mbox{Transpose} & \lmtrans{\cdot} & : & (s \linto t) \to (t \linto s) & \mbox{Matrix transpose} \\
%     \mbox{Lambda} & \lmlam{\cdot} & : & (\nat \to (s \linto t)) \to (s \linto (\nat \to t)) \\
%     \mbox{TLambda} & \lmlamt{\cdot} & : & (\nat \to (t \linto s)) \to ((\nat \to t) \linto s) \\
    \mbox{Build}   & \lmbuild  & : & (\nat,\; \nat \to (s \linto t)) \to (s \linto \vector{t}) \\
    \mbox{BuildT}   & \lmbuildt  & : & (\nat,\; \nat \to (t \linto s)) \to (\vector{t} \linto s) \\
  \end{array}
  $$
\end{minipage}}
  \caption{Operations over linear maps} \label{fig:linear-maps}
\end{figure*}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
{\bf Semantics of linear maps}
  $$
  \begin{array}{rcl}
    (m_1 \lmcomp m_2) \lmapply x  & = & m_1 \lmapply (m_2 \lmapply x) \\
    (m_1 \lmpair m_2) \lmapply x  & = & (m_1 \lmapply x, m_2 \lmapply x) \\
    (m_1 \lmjoin m_2)  \lmapply (x_1,x_2) & = & (m_1 \lmapply x_1) + (m_2 \lmapply x_2) \\
    (m_1 \lmadd m_2)  \lmapply x & = & (m_1 \lmapply x) + (m_2 \lmapply x) \\
    \lmzero \lmapply x  & = & 0 \\
    \lmone \lmapply x & = & x \\
    \lmscalar{k} \lmapply x & = & k * x \\
%    \lmlam{f} \lmapply x & = & \lambda n\dot (f n) \lmapply x \\
%    \lmlamt{f} \lmapply g & = & \Sigma_i f(i) \lmapply g(i) \\
    \lmbuild(n,f) \lmapply x & = & build(n, \lambda i. (f\; i) \lmapply x) \\
    \lmbuildt(n,f) \lmapply x & = & \sumfun\, \buildfun(n,\; \lambda i. (f\; i) \lmapply x[i] ) \\
   \end{array}
  $$
\\[3mm]
  {\bf Rules for transposition of linear maps}
  $$
  \begin{array}{rcll}
    \lmtrans{(m_1 \lmcomp m_2)} & = & \lmtrans{m_2} \lmcomp \lmtrans{m_1} & \mbox{Note reversed order!}\\
    \lmtrans{(m_1 \lmpair m_2)} & = & \lmtrans{m_1} \lmjoin \lmtrans{m_2} \\
    \lmtrans{(m_1 \lmjoin m_2)} & = & \lmtrans{m_1} \lmpair \lmtrans{m_2} \\
    \lmtrans{(m_1 \lmadd m_2)} & = & \lmtrans{m_1} \lmadd \lmtrans{m_2} \\
    \lmtrans{\lmzero} & = & \lmzero \\
    \lmtrans{\lmone} & = & \lmone \\
    \lmtrans{\lmscalar{k}} & = & \lmscalar{k} \\
    \lmtrans{(\lmtrans{m})} & = & m \\
    \lmtrans{\lmbuild( n,\; \lambda i. m )} & = & \lmbuildt( n,\; \lambda i. \lmtrans{m} ) \\
    \lmtrans{\lmbuildt( n,\; \lambda i. m )} & = & \lmbuild( n,\; \lambda i. \lmtrans{m} ) \\
%    \lmtrans{\lmlam{\lambda i. m}} & = & \lmlamt{\lambda i. \lmtrans{m}} \\
%    \lmtrans{\lmlamt{\lambda i. m}} & = & \lmlam{\lambda i. \lmtrans{m}} \\
  \end{array}
  $$
\\[3mm]
  {\bf Laws for linear maps}
  $$
  \begin{array}{rcl}
    \lmzero \lmcomp m & = & \lmzero \\
    m \lmcomp \lmzero & = & \lmzero \\
    \lmone \lmcomp m & = & m \\
    m \lmcomp \lmone & = & m \\
    m \lmadd \lmzero & = & m \\
    \lmzero \lmadd m & = & m \\
    m \lmcomp (n_1 \lmjoin n_2) & = & (m \lmcomp n_1) \lmjoin (m \lmcomp n_2) \\
    (m_1 \lmjoin m_2) \lmcomp (n_1 \lmpair n_2) & = & (m_1 \lmcomp n_1) \lmadd (m_2 \lmcomp n_2) \\
    \lmscalar{k_1} \lmcomp \lmscalar{k_2} & = & \lmscalar{ k_1 * k_2 } \\
    \lmscalar{k_1} \lmadd \lmscalar{k_2} & = & \lmscalar{ k_1 + k_2 } \\
  \end{array}
  $$
    \end{minipage}
    }
    \caption{Laws for linear maps} \label{fig:lm-laws}
\end{figure}

A \emph{linear map}, $m : S \linto T$, is a function from $S$ to $T$,
satisfying these two properties:
$$
\begin{array}{rrcl}
  \forall \typ{x,y}{S} &  m \lmapply (x+y) & = & m \lmapply x + m \lmapply y \\
  \forall \typ{k}{\real}, \typ{x}{S} & k * (m \lmapply x) & = & m \lmapply (k * x)
\end{array}
$$
Here $(\lmapply) :: (s \linto t) \to (s \to t)$ is an operator that applies a linear map $(s \linto t)$
to an argument of type $s$.

\begin{itemize}
  \item Linear maps can be \emph{built} using the operators in (see Figure~\ref{fig:linear-maps}).
  \item The \emph{semantics} of a linear map is completely specified by saying
    what ordinary function it corresponds to; or, equivalently, by how it behaves when applied
    to an argument by $(\lmapply)$.  The semantics of each form of linear map are given in Figure~\ref{fig:lm-laws}
  \item Linear maps satisfy \emph{laws} given in Figure~\ref{fig:lm-laws}.  Note that $(\lmcomp)$ and $\lmadd$ behave
    like multiplication and addition respectively.
\end{itemize}

\subsection{Matrix interpretation of linear maps}

A linear map $m :: \real^m \linto \real^n$ is isomorphic to an matrix $\real^{n \times m}$ with $n$ rows and $m$ columns.

\subsection{Questions about linear maps}

\begin{itemize}
\item Do we need $\lmone$? After all $\lmscalar{1}$ does the same job.  But asking if $k=1$ is dodgy when $k$ is a float.
\item Do these laws fully define linear maps?
\item How do we transpose $\lmbuild$?
\end{itemize}
Notes
\begin{itemize}
\item In practice we allow n-ary versions of $m \lmjoin n$ and $m \lmpair n$.
\end{itemize}

% ------------------------------------
\section{The language}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
$$
      \begin{array}{rcll}
        \multicolumn{4}{l}{\mbox{\bf Atoms}} \\
        f,g,h & ::= & \multicolumn{2}{l}{\mbox{Function}} \\
        x,y,z & ::= & \multicolumn{2}{l}{\mbox{Local variable (lambda-bound or let-bound)}} \\
        k & ::= & \multicolumn{2}{l}{\mbox{Literal constants}} \\
        \\
        \multicolumn{4}{l}{\mbox{\bf Terms}} \\
        \mathit{pgm} & ::= & \mathit{def}_1 \ldots \mathit{def}_n \\
        \mathit{def} & ::= & f(x) = e \\
        e & ::= & k & \mbox{Constant} \\
          & |   & x & \mbox{Local variable} \\
          & |   & f(e) & \mbox{Function call} \\
          & |   & (e_1,e_2) & \mbox{Pair} \\
          & |   & \lambda x \dot e & \mbox{Lambda} \\
          & |   & e_1 \; e_2 & \mbox{Application} \\
          & |   & \mbox{\lstinline|let $x$ = $e_1$ in $\;e_2$|} \\
        \\
        \multicolumn{4}{l}{\mbox{\bf Types}} \\
        \tau & ::= & \nat & \mbox{Natural numbers} \\
        & | & \real & \mbox{Real numbers} \\
        & | & (\tau_1, \tau_2) & \mbox{Pairs} \\
        & | & \vector{\tau} & \mbox{Vectors} \\
        & | & \tau_1 \to \tau_2 & \mbox{Functions} \\
        & | & \tau_1 \linto \tau_2 & \mbox{Linear maps} \\
      \end{array}
 $$
\end{minipage}}
\caption{Syntax of the language} \label{fig:syntax}
\end{figure}
The syntax of our intermediate language is given in Figure~\ref{fig:syntax}.
Note that
\begin{itemize}
\item  Variables are divided into \emph{functions}, $f,g,h$; and \emph{local variables}, $x,y,z$,
  which are either function arguments or let-bound.
\item 
  The language has a first order sub-language.  Functions are defined at top level;
  functions always appear in a call, never (say) as an argument to a
  vunction; in a call $f(e)$, the function $f$ is always a
  top-level-defined function, never a local variable.

\item Functions have exactly one argument. If you want more than one, pass a pair.

\item Pairs are built-in, with selectors $\sel{1}{2}, \sel{2}{2}$.
  (In the real implementation, pairs are generalised to $n$-tuples.)

\item Conditionals are handled by a function $\iffun$.

\item Let-bindings are non-recursive. For now, at least, top-level
  functions are also non-recursive.  \simon{I think that top-level
    recursive functions might be OK, but I don't want to think about
    that yet.}

\item Lambda expressions and applicatons are are present, so the language
  is higher order.  AD will only accept a subset of the language, in
  which lambdas appear only as an arguemnt to $\buildfun$.  But the
  \emph{output} of AD may include lambdas and application, as we shall see.
  \end{itemize}

\subsection{Built in functions}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
{\bf Built-in functions}
$$
\begin{array}{rcll}
  (+) & :: & Field \; t \darrow (t,t) \to t \\
  (*) & :: & Field \; t \darrow (t,t) \to t \\
  \sel{1}{2} & :: & (t_1,t_2) \to t_1 & \mbox{Selection} \\
  \sel{2}{2} & :: & (t_1,t_2) \to t_2 & \mbox{..ditto..} \\
  \deltafun & :: & Field \; t \darrow (\nat,\; \nat) \to t & \mbox{Delta-function} \\
  \buildfun & :: & (\nat,\; \nat \to t) \to \vector{t} & \mbox{Vector build} \\
  \indexfun{\cdot}{\cdot} & :: & (\vector{t},\; \nat) \to t & \mbox {Indexing} \\
  \sumfun & :: & Field \; t \darrow \vector{t} \to t & \mbox{Sum a vector} \\
  \iffun & :: & Bool \to r \to r \to r \\
\end{array}
$$
\\[3mm]
{\bf Derivatives of built-in functions}
      $$
      \begin{array}{rcl}
        \gradf{+}      & :: & Field\; t \darrow (t,t) \to ((t,t) \linto t) \\
        \gradf{+}(x,y) & = & \lmone \lmjoin \lmone \\[2mm]
        \gradf{*}      & :: & Field\; t \darrow (t,t) \to ((t,t) \linto t) \\
        \gradf{*}(x,y) & = & \lmscalar{y} \lmjoin \lmscalar{x} \\[2mm]
        \gradf{\sel{1}{2}}      & :: & (t,t) \to ((t,t) \linto t) \\
        \gradf{\sel{1}{2}}(x) & = & \lmone \lmjoin \lmzero \\[2mm]
        \gradf{\indexfun{\cdot}{\cdot}} & :: & (\vector{t},\; \nat) \to ((\vector{t},\; \nat) \linto t) \\
        \gradf{\indexfun{\cdot}{\cdot}}(v,i) & = & \lmbuildt( \sizefun(v), \lambda j. \deltafun(i,j)) \lmjoin \lmzero \\[2mm]
        \gradf{\sumfun} & :: & Field \; t \darrow \vector{t} \to (\vector{t} \linto t) \\
        \gradf{\sumfun}(v) & = & \lmbuildt( \sizefun(v),\; \lambda i. \lmone ) \\[2mm]
        \gradf{\iffun} & :: & (Bool,r,r) \to (((Bool,r,r) \linto r)) \\
        \gradf{\iffun}(True,t,f) & = & \lmzero \lmjoin \lmone \lmjoin \lmzero \\
        \gradf{\iffun}(False,t,f) & = & \lmzero \lmjoin \lmzero \lmjoin \lmone \\
        \ldots
        \end{array}
$$
\end{minipage}}
\caption{Built-in functions} \label{fig:built-in}
\end{figure}

We assume built-in functions shown in Figure~\ref{fig:built-in}.

We allow ourselves to write functions infix where it is convenient.
Thus $e_1 + e_2$ means the call $+(e1,e2)$, which applies the function $+$ to
the pair $(e_1,e_2)$.  (So, like all other functions, $(+)$ has one argument.)
Similarly the linear map $m_1 \lmpair m_2$ is short for $\lmpair(e_1,e_2)$.

We allow ourselves to write vector indexing using square brackets, thus $a[i]$.

Multiplication and addition are overloaded to work on any suitable type.
On vectors they work element-wise; if you want dot-product you have to program it.

The delta-function $\deltafun$ is defined thus:
$$
\begin{array}{rcll}
  \deltafun(i,j) & = & 1 & \mbox{if $i=j$} \\
  & = & 0 & \mbox{otherwise}
\end{array}
$$
\section{Automatic differentiation}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
$$
\begin{array}{ll}
  \mbox{\bf Original function}   & f : S \to T \\
  & f(x) = e \\[2mm]
  \mbox{\bf Full Jacobian}       & \gradf{f}  :  S \to (S \linto T) \\
  & \mbox{\lstinline|$\gradf{f}(x)$ = let $\;\gradf{x}$ = $\lmone\;$ in $\;\grad{e}$|} \\[2mm]
  \mbox{\bf Transposed Jacobian}   & \gradft{f}  :  S \to (T \linto S) \\
  & \mbox{\lstinline|$\gradft{f}(x)$ =  $\lmtrans{(\gradf{f}(x))}$|}  \\[2mm]
  \mbox{\bf Forward derivative}  & \fwdDf{f} : (S,S) \to T \\
  & \fwdDf{f}(x,dx) = \gradf{f}(x) \lmapply  dx \\[2mm]
  \mbox{\bf Reverse derivative}  & \revDf{f} : (S,T) \to S \\
  & \revDf{f}(x,dr) = \gradft{f}(x) \lmapply dr
\end{array}
$$
      {\bf Differentiation of an expression} \\
      \begin{center}
        If $e :: T$ then $\grad{e} :: S \linto T$
      \end{center}
$$
      \begin{array}{rcll}
        \grad{k} & = & \lmzero \\
        \grad{x} & = & \gradf{x} \\
        \grad{f(e)} & = & \gradf{f}(e) \lmcomp \grad{e} \\
        \grad{(e_1,e_2)} & = & \grad{e_1} \lmpair \grad{e_2} \\
        \grad{\buildfun(e_n, \lambda i.e)} & = & \lmbuild(e_n, \lambda i. \grad{e}) \\
%         \grad{\lambda x \dot e} & = & \lmlam{\lambda x\dot \grad{e}} \\
        \grad{\mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|}}
        & = & \begin{array}[t]{l}
           \mbox{\lstinline|let $\;x\;$ = $\;e_1\;$ in|} \\
           \mbox{\lstinline|let $\;\gradf{x}\;$ = $\;\grad{e_1}\;$ in|} \\
           \mbox{\lstinline|$\grad{e_2}$|}
           \end{array}
      \end{array}
      $$
\end{minipage}}
\caption{Automatic differentiation} \label{fig:ad}
\end{figure}

To perform source-to-source AD of a function $f$, we follow the plan
outlined in Figure~\ref{fig:ad}.  Specifically, starting with a
function definition \lstinline|f(x) = e|:

\begin{itemize}
\item Construct the full Jacobian $\gradf{f}$, and transposed full Jacobian $\gradft{f}$,
  using the tranformations in Figure~\ref{fig:ad}.
\item Optimise these two definitions, using the laws of linear maps
  in Figure~\ref{fig:lm-laws}.
\item Construct the forward derivative $\fwdDf{f}$ and reverse derivative $\revDf{f}$,
  as shown in Figure~\ref{fig:ad}.
\item Optimise these two definitions, to eliminate all linear maps. Specifically:
  \begin{itemize}
    \item Rather than \emph{calling} $\gradf{f}$ (in, say, $\fwdDf{f}$), instead \emph{inline} it.
    \item Similarly, for each local let-binding for a linear map, of form \lstinline|let $\;\gradf{x}$ = $e\;$ in $b$|,
      inline $\gradf{x}$ at each of its occurrences in $b$. This may duplicate $e$; but $\gradf{x}$ is a function
      that may be applied (via $\lmapply$) to many different arguments, and we want to specialise it for each
      such call.  (I think.)
    \item Optimise using the rules of $(\lmapply)$ in Figure~\ref{fig:lm-laws}.
    \item Use standard Common Subexpression Elimination (CSE)to recover any lost sharing.
  \end{itemize}
\end{itemize}

Note that
\begin{itemize}
\item The transformation is fully compositional; each function can be AD'd independently.
  For example, if a user-defined
  fuction $f$ calls another user-defined function $g$, we construct $\gradf{g}$ as
  described; and then construct $\gradf{f}$. The latter simply calls $\gradf{g}$.

\item The AD transformation is \emph{partial}; that is, it does not work for every
  program. In particular, it fails when applield to a lambda, or an application; and,
  as we will see in Seciton~\ref{sec:vectors}, it requires that $\buildfun$ appears
  applied to a lambda.

\item We give the full Jacobian for some built-in functions in Figure~\ref{fig:ad}, including
  for conditionals ($\gradf{\iffun}$).
\end{itemize}

\subsection{Avoiding duplication}

We may want to ANF-ise before AD to avoid gratuitous duplication.
  E.g.
$$
  \begin{array}{rcl}
    \multicolumn{3}{l}{\grad{sqrt(x+(y*z))}} \\
      & = & \gradf{sqrt}(x+(y*z)) \lmcomp \grad{x+(y*z)} \\
    & = & \gradf{sqrt}(x+(y*z)) \lmcomp  \gradf{+}(x, y*z) \\
     && \lmcomp (\grad{x} \lmpair \grad{y*z}) \\
    & = & \gradf{sqrt}(x+(y*z)) \lmcomp \gradf{+}(x, y*z) \\
    & & \lmcomp (\gradf{x} \lmpair (\gradf{*}(y,z) \lmcomp (\gradf{y} \lmpair \gradf{z}))) \\
  \end{array}
  $$
Note the duplication of $y*z$ in the result.
Of course, CSE may recover it.

\subsection{Vectors}

The language supposts one-dimensional vectors, of type $\vector{T}$,
whose elements have type $T$ (Figure~\ref{fig:syntax}).
It has built-in functions (Figure~\ref{fig:built-in}):
\begin{itemize}
\item $\buildfun :: (\nat,\; \nat \to t) \to \vector{t}$ for vector construction.
\item $\indexfun{\cdot}{\cdot} :: (\vector{t},\; \nat) \to t$ for indexing.
\item $\sumfun :: Field \; t \darrow \vector{t} \to t$ to add up the elements of a vector.
\item $\sizefun :: \vector{t} \to \nat$ takes the size of a vector.
\item Arithmetic, $(*), (+)$ etc is overloaded to work over vectors, always elementwise.
\end{itemize}
All of these function have their full Jacobian versions, which are also
defined in Figure~\ref{fig:built-in}.  
You may enjoy checking that $\gradf{\sumfun}$ and
$\gradf{\indexfun{\cdot}{\cdot}}$ are correct!  I did think about having
a specialised linear map for indexing, rather than using $\lmbuildt$,
but then I needed its transposition, and that needed $\deltafun$ anyway.

But $\buildfun$ is an exception!  It is handled specially
by the AD transformation in Figure~\ref{fig:ad}; there is no $\gradfun{\buildfun}$.
Moreover the AD transformation only works if the second argument of the build is
a lambda, thus $\buildfun(e_n, \lambda i.e)$.  I tried dealing with build and
lambdas separately, but failed (see Section~\ref{sec:build-lam-fail}).

\subsection{General folds}

We have $\sumfun :: \vector{\real} \to \real$.  What is $\gradf{\sumfun}$?
One way to define its semantics is by applying it:
$$
\begin{array}{rcl}
  \gradf{\sumfun} & :: & \vector{\real} \to (\vector{\real} \linto \real) \\
  \gradf{\sumfun}(v) \lmapply dv & = & \sumfun(dv)
\end{array}
$$
That is OK.  But what about product, which multiplies all the elements
of a vector together? If the vector had three elements we might have
$$
\begin{array}{l}
  \gradf{product}([x_1,x_2,x_3]) \lmapply [dx_1, dx_2, dx_3] \\
  \quad = (dx_1 * x_2 * x_3) + (dx_2 * x_1 * x_3) + (dx_3 * x_1 * x_2)
\end{array}
$$
This looks very unattractive as the number of elements grows.  Do we need
to use product?

This gives the clue that taking the derivative of $\mathit{fold}$ is
not going to be easy, maybe infeasible!  Much depends on the
particular lambda it appears.  So I have left out product, and made
no attempt to do general folds.

\section{Implementation}

The implementation differs from this document as follows:
\begin{itemize}
\item Rather than pairs, the implementation supports $n$-ary tuples.
  Similary the linear maps $(\lmpair)$ and $\lmjoin$ are $n$-ary.
\item Functions definitions can take $n$ arguments, thus
  \begin{lstlisting}
   f(x,y,z) = e
  \end{lstlisting}
  This is treated as equivalent to
  \begin{lstlisting}
    f(t) = let x = $\sel{1}{3}$(t)
               y = $\sel{2}{3}$(t)
               z = $\sel{3}{3}$(t)
           in e
  \end{lstlisting}
\end{itemize}

\section{Demo}

You can run the prototype by saying {\tt ghci Main}.

The function {\tt demo :: Def -> IO ()} runs the
prototype on the function provided as example.
Thus:
{\small
\begin{verbatim}
bash$ ghci Main

*Main> demo ex2

----------------------------
Original definition
----------------------------
fun f2(x)
  = let { y = x * x }
    let { z = x + y }
    y * z

----------------------------
Anf-ised original definition
----------------------------
fun f2(x)
  = let { y = x * x }
    let { z = x + y }
    y * z

----------------------------
The full Jacobian (unoptimised)
----------------------------
fun Df2(x)
  = let { Dx = lmOne() }
    let { y = x * x }
    let { Dy = lmCompose(D*(x, x), lmVCat(Dx, Dx)) }
    let { z = x + y }
    let { Dz = lmCompose(D+(x, y), lmVCat(Dx, Dy)) }
    lmCompose(D*(y, z), lmVCat(Dy, Dz))

----------------------------
The full Jacobian (optimised)
----------------------------
fun Df2(x)
  = let { y = x * x }
    lmScale( (x + y) * (x + x) + (x + y) * (x + x) )

----------------------------
Forward derivative (unoptimised)
----------------------------
fun f2'(x, dx)
  = lmApply(let { y = x * x }
            lmScale( (x + y) * (x + x) +
                     (x + y) * (x + x) ),
            dx)

----------------------------
Forward-mode derivative (optimised)
----------------------------
fun f2'(x, dx)
  = let { y = x * x }
    ((x + y) * (x + x) + (x + y) * (x + x)) * dx

----------------------------
Forward-mode derivative (CSE'd)
----------------------------
fun f2'(x, dx)
  = let { t1 = x + x * x }
    let { t2 = x + x }
    (t1 * t2 + t1 * t2) * dx

----------------------------
Transposed Jacobian
----------------------------
fun Rf2(x)
  = lmTranspose( let { y = x * x }
                 lmScale( (x + y) * (x + x) +
                         (x + y) * (x + x) ) )

----------------------------
Optimised transposed Jacobian
----------------------------
fun Rf2(x)
  = let { y = x * x }
    lmScale( (x + y) * (x + x) +
             (x + y) * (x + x) )

----------------------------
Reverse-mode derivative (unoptimised)
----------------------------
fun f2`(x, dr)
  = lmApply(let { y = x * x }
            lmScale( (x + y) * (x + x) +
                     (x + y) * (x + x) ),
            dr)

----------------------------
Reverse-mode derivative (optimised)
----------------------------
fun f2`(x, dr)
  = let { y = x * x }
    ((x + y) * (x + x) +
     (x + y) * (x + x)) * dr

----------------------------
Reverse-mode derivative (CSE'd)
----------------------------

fun f2`(x, dr)
  = let { t1 = x + x * x }
    let { t2 = x + x }
    (t1 * t2 + t1 * t2) * dr
\end{verbatim}
}


\section{Old stuff about Build and lambda} \label{sec:build-lam-fail}

I'm not happy with the story for build and lambda. It started well:
\begin{itemize}
\item For a start, the treatment of lambda is very special: it works only
  for $(\nat \to t)$, which seems oddly assymetrical.

\item The AD for build (Fig~\ref{fig:ad}) looks sensible.
  But not great:  rather than use the generic case for a call $f(e)$,
  I used a special case for $\grad{\buildfun(e_n,e)}$; and I did
  an ad-hoc thing of not AD'ing the first argument to build, which
  seems very arbitrary.  The AD's version of build uses $\lmbuild$, whose
  signature is in Figure~\ref{fig:linear-maps}; and whose semantics is
  defined by how it behaves when applied (Figure~\ref{fig:lm-laws}).

\item The AD for lambda (Figure~\ref{fig:ad}) looks sensible.  It generates
  a new linear map $\lmlam$, whose
  signature is in Figure~\ref{fig:linear-maps}; and whose semantics is
  defined by how it behaves when applied (Figure~\ref{fig:lm-laws}).

\item But then things get trickier.  What is the transose of $\lmlam$?
  I invented $\lmlamt$ (Figure~\ref{fig:linear-maps}) as its transpose.
  There is a nice pattern here: $\lmpair$ and $\lmjoin$ are related
  in just the same way as $\lmlam$ and $\lmlamt$.

\item But I got stuck: what is the semantics of $\lmlamt$?
  There is a stab in Figure~\ref{fig:lm-laws}, but the $\Sigma_i$ is deeply
  suspicious, because it doesn't give the range of $i$.  That range comes
  from the enclosing build.
\end{itemize}

An alternative is to replace lambda with
$$
e ::= \ldots | \buildfun(n) i e
$$
where $i$ is a variable (of type $\nat$) that scopes over $e$.
So $\buildfun(n)~ i~ e$ is what we have been writing $\buildfun(n, \lambda i.e)$.
This looks simpler and more direct to me.

\end{document}

do  C( s -o t) -> s -o C(t)
un             -> C(s) -o t

Given    m :: s -o (N -> t)
Wanted   mt :: (N -> t) -o s

L  :: (N -> (s -o t)) -> s -o (N -> t)
Lt ::                 -> (N -> s) -o t
